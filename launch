#!/usr/bin/env python3

import sys

assert sys.version_info >= (3, 6), "Python >= 3.6 required"

import os
import pandas as pd
import stat
import re
import argparse
import subprocess
from random import randrange


exp_def = 2000
cutoff_def = 15
mf_def = 3

parser = argparse.ArgumentParser()
parser.add_argument('--dataset', '-d', type=str, required=True,
	help="dataset directory")
parser.add_argument('--experiments', '-e', type=int, default=exp_def,
	help="experiment cutoff time in minutes")
parser.add_argument('--cutoff', '-c', type=int, default=cutoff_def,
	help="experiment cutoff time in minutes")
parser.add_argument('--metafolds', '-m', type=int,
	choices=(1,3), default=mf_def,
	help="number of metafolds provided for bottom-level sampling")
parser.add_argument('--repeat', '-r', type=int, default=1,
	help="launch or validate n repetitions")
parser.add_argument('--slurm', action='store_true',
	help="run in slurm-controlled environment using sbatch (default False)")
parser.set_defaults(slurm=False)

group = parser.add_mutually_exclusive_group()
group.add_argument('--validate', '-v', type=int,
	help="validate completed run (pass job id of run)")
group.add_argument('--integrate', '-i', type=int,
	help="integrate validation (pass job id of run)")

args = parser.parse_args()
ds = args.dataset
exp = args.experiments
cutoff = args.cutoff
mf = args.metafolds
slurm = args.slurm

log_name = "{}-{}f{}k{}m-irace".format(ds, mf, exp//1000, cutoff)

if not args.validate and not args.integrate:
	print("launching {}: {} experiments, {}m cutoff, {} MFs".format(
		ds, exp, cutoff, mf))
	if args.repeat > 1:
		print(args.repeat, "times")

	log_name = "{}-{}f{}k{}m-irace".format(ds, mf, exp//1000, cutoff)

	os.makedirs("output/"+ds, exist_ok=True)

	if slurm:
		output = "output/{}/isklearn-{}-%j.out".format(ds, log_name)
	else:
		job_id = ''.join([str(randrange(10)) for _ in range(6)])
		output = ["output/{}/isklearn-{}-{}{}.out".format(ds, log_name,
			job_id, repetition) for repetition in range(args.repeat)]

	for r in range(args.repeat):
		if slurm:
			cmd = ["sbatch", "--output", output, "batch_script",
				ds] + list(map(str, [exp, cutoff, mf])) + [log_name]
			subprocess.run(cmd)
		else:
			cmd = ["./batch_script", ds] + list(map(str,
				[exp, cutoff, mf])) + [log_name]
			with open(output[r], 'w') as output_file:
				subprocess.run(cmd, stdout=output_file, stderr=output_file)
		
else:
	first_id = args.validate if args.validate else args.integrate
	val_dir = "validation/{}-{}".format(log_name, first_id)
	if args.repeat > 1:
		val_dir += "-{}".format(first_id+args.repeat-1)
		
	if args.validate:
		os.makedirs(val_dir, exist_ok=True)

		for job_id in range(first_id, first_id+args.repeat):
			# get elite configurations
			configs = []
			out_log = "output/{}/isklearn-{}-{}.out".format(
				ds, log_name, job_id)
			with open(out_log, 'r') as log_file:
				config_re = r'^\d+\s+--dataset.+'
				results = [re.findall(config_re, line) for line in log_file]
				configs = [config for result in results for config in result]
			if not configs:
				print("WARNING: No elite configs found for "+out_log)
				continue

			# generate validation script
			script_file = val_dir+"/val-script-{}".format(job_id)

			with open(script_file, 'w') as script:
				script.write("#!/bin/bash\n")
				if slurm:
					script.write("#SBATCH --time=1-0:00\n")
					script.write("#SBATCH --cpus-per-task=1\n")
					script.write("#SBATCH --mem-per-cpu=16G\n")
					script.write("#SBATCH --array=0-{}\n".format(len(configs)-1))
					script.write("#SBATCH --job-name={}-val\n".format(log_name))
					script.write("#SBATCH --output={}/{}-{}-val-%A-%a\n".format(
						val_dir, log_name, job_id))

					script.write("\nCONFIGS=(")
					for config in configs:
						script.write("\"{}\"\n".format(config))
					script.write(")\n\n")

					val_cmd = "srun python {}/{}/validation.py".format(
						os.getcwd(), ds)
					script.write(val_cmd+" ${CONFIGS[$SLURM_ARRAY_TASK_ID]}\n")
				else:
					curr_dir = os.getcwd()
					val_id = ''.join([str(randrange(10)) for _ in range(6)])
					for r, config in enumerate(configs):
						val_out_file = "{}/{}-{}-val-{}-{}\n".format(
							val_dir, log_name, job_id, val_id, r)
						val_cmd = "python {}/{}/validation.py".format(
							curr_dir, ds)
						script.write("{} {} &> {}\n".format(val_cmd,
							config, val_out_file))
			# make script executable
			st = os.stat(script_file)
			os.chmod(script_file, st.st_mode | stat.S_IEXEC)

			# run script
			cmd = ["sbatch", script_file] if slurm else [script_file]
			subprocess.run(cmd)
	else:
		best_file = "{}/{}-best.out".format(val_dir, log_name)
		open(best_file, 'w').close()

		all_best_args = []
		for job_id in range(first_id, first_id+args.repeat):
			val_base = "{}-{}-val".format(
				log_name, job_id)
			val_re = re.compile(val_base + r"-\d+-\d+")
			val_files = [val_dir + "/" + f for f in filter(
				val_re.search, os.listdir(val_dir))]

			full_file = "{}/{}.out".format(val_dir, val_base)
			with open(full_file, 'w') as outfile:
				for fname in val_files:
					with open(fname) as infile:
						outfile.write(infile.read())
					# os.remove(fname)

			with open(best_file, 'a') as best:
				with open(full_file, 'r') as infile:
					result_re = r'(\[[\d\.,\s]+\] ([\d\.]+)\n.+)'
					results = re.findall(result_re, infile.read())
					if not results:
						print("WARNING: No validation results in", full_file)
						continue
					br = sorted(results, key=lambda r: float(r[1]),
						reverse=True)[0][0]
					best_args = br.split('\n')[1].split(' ')

					parser = argparse.ArgumentParser()
					parser.add_argument('--pre_scaling', default="False")
					parser.add_argument('--f_eng1', required=True)
					parser.add_argument('--f_eng2', required=True)
					parser.add_argument('--scaling', required=True)
					parser.add_argument('--algorithm', required=True)
					best_args = parser.parse_known_args(args=best_args)[0]

					for a in ('f_eng1', 'f_eng2'):
						v = getattr(best_args, a)
						if not v or v in ('None', 'none'):
							setattr(best_args, a, 'None')

					for i, a in enumerate(('pre_scaling', 'f_eng1',
							'f_eng2', 'scaling', 'algorithm')):
						best_args_dict = {
							'dataset': ds,
							'metafolds': mf,
							'experiments': exp,
							'cutoff': cutoff,
							'seed': job_id,
							'stage': 'preprocessing' if i < 3 else 'prediction',
							'component': a,
							'value': getattr(best_args, a)
						}
						all_best_args.append(best_args_dict)
					best.write(br + "\n\n")
		best_df = pd.DataFrame(all_best_args)
		best_df.to_csv("{}/{}-best-args.csv".format(val_dir, log_name), index=False)
		print("Results compiled in", best_file)

